{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e59a05d-5251-4dfe-9fde-bd8a7def5fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to enhanced_predictive_maintenance_dataset.csv\n",
      "Dataset shape: (10000, 11)\n",
      "Failure rate: 55.80%\n",
      "\n",
      "Dataset columns: ['equipment_id', 'timestamp', 'temperature', 'vibration', 'pressure', 'runtime_hours', 'current', 'noise_level', 'oil_quality', 'wear_rate', 'failure']\n",
      "\n",
      "First few rows of the dataset:\n",
      "  equipment_id           timestamp  temperature  vibration    pressure  \\\n",
      "0      EQ00000 2024-01-01 00:00:00    72.611415   0.417893  107.486261   \n",
      "1      EQ00001 2024-01-01 00:30:00    68.635069   0.478238  106.457127   \n",
      "2      EQ00002 2024-01-01 01:00:00    76.190049   0.407369   83.809437   \n",
      "3      EQ00003 2024-01-01 01:30:00    82.785237   0.505103  106.017007   \n",
      "4      EQ00004 2024-01-01 02:00:00    66.507858   0.650506   67.944531   \n",
      "\n",
      "   runtime_hours    current  noise_level  oil_quality  wear_rate  failure  \n",
      "0    1783.009551  16.447449    45.750111    24.851429   0.025821        0  \n",
      "1    2134.056179  15.114928    58.441357     9.652947   0.008965        0  \n",
      "2     567.837737   9.290866    64.262144    35.299754   0.000000        0  \n",
      "3     469.504587  13.302683    70.500068    34.799220   0.011904        0  \n",
      "4     948.552974  14.019647    51.099810    55.061160   0.002582        0  \n",
      "\n",
      "Basic statistics:\n",
      "                 timestamp   temperature     vibration      pressure  \\\n",
      "count                10000  10000.000000  10000.000000  10000.000000   \n",
      "mean   2024-04-14 03:45:00     70.018817      0.501250     99.790243   \n",
      "min    2024-01-01 00:00:00     30.749090      0.084733     27.997745   \n",
      "25%    2024-02-22 01:52:30     63.158697      0.432003     85.979682   \n",
      "50%    2024-04-14 03:45:00     69.948440      0.500868     99.868464   \n",
      "75%    2024-06-05 05:37:30     76.842428      0.570350    113.713432   \n",
      "max    2024-07-27 07:30:00    113.074234      0.933317    172.763322   \n",
      "std                    NaN     10.199334      0.102236     20.489403   \n",
      "\n",
      "       runtime_hours       current   noise_level   oil_quality     wear_rate  \\\n",
      "count   10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
      "mean     1002.364807     15.020091     60.007530     28.748346      0.009862   \n",
      "min         0.016736      2.401218     20.752809      0.000000      0.000000   \n",
      "25%       282.206461     12.968351     53.186379     16.210889      0.002907   \n",
      "50%       685.931390     15.046370     60.037162     26.994601      0.006944   \n",
      "75%      1404.669839     17.062465     67.056076     39.314802      0.013738   \n",
      "max     10488.583089     25.641337     99.961585     88.801752      0.094063   \n",
      "std      1010.166954      3.018261     10.307138     16.493603      0.009970   \n",
      "\n",
      "            failure  \n",
      "count  10000.000000  \n",
      "mean       0.558000  \n",
      "min        0.000000  \n",
      "25%        0.000000  \n",
      "50%        1.000000  \n",
      "75%        1.000000  \n",
      "max        1.000000  \n",
      "std        0.496649  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_enhanced_dataset(n_samples=10000, save_path=\"enhanced_predictive_maintenance_dataset.csv\"):\n",
    "    \"\"\"\n",
    "    Generate an enhanced synthetic dataset for predictive maintenance with additional metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    n_samples (int): Number of data samples to generate\n",
    "    save_path (str): Path to save the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Generated dataset\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate synthetic data with expanded metrics\n",
    "    data = {\n",
    "        'equipment_id': [f'EQ{str(i).zfill(5)}' for i in range(n_samples)],\n",
    "        'timestamp': pd.date_range(start='2024-01-01', periods=n_samples, freq='30min'),\n",
    "        'temperature': np.random.normal(70, 10, n_samples),      # Â°F, normal operation ~70\n",
    "        'vibration': np.random.normal(0.5, 0.1, n_samples),     # mm/s\n",
    "        'pressure': np.random.normal(100, 20, n_samples),       # psi\n",
    "        'runtime_hours': np.random.exponential(1000, n_samples),# hours\n",
    "        'current': np.random.normal(15, 3, n_samples),          # Amps, motor current\n",
    "        'noise_level': np.random.normal(60, 10, n_samples),     # dB, equipment noise\n",
    "        'oil_quality': np.random.beta(2, 5, n_samples) * 100,   # % cleanliness, 0-100\n",
    "        'wear_rate': np.random.exponential(0.01, n_samples),    # mm/month\n",
    "        'failure': np.zeros(n_samples, dtype=int)\n",
    "    }\n",
    "    \n",
    "    # Simulate failure conditions based on expanded metrics\n",
    "    for i in range(n_samples):\n",
    "        if (data['temperature'][i] > 85 or \n",
    "            data['vibration'][i] > 0.7 or \n",
    "            data['pressure'][i] > 130 or \n",
    "            data['runtime_hours'][i] > 1500 or\n",
    "            data['current'][i] > 20 or\n",
    "            data['noise_level'][i] > 80 or\n",
    "            data['oil_quality'][i] < 30 or\n",
    "            data['wear_rate'][i] > 0.05):\n",
    "            data['failure'][i] = 1 if np.random.rand() > 0.25 else 0\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add realistic noise to simulate data imperfections\n",
    "    df['temperature'] += np.random.normal(0, 2, n_samples)\n",
    "    df['vibration'] += np.random.normal(0, 0.02, n_samples)\n",
    "    df['pressure'] += np.random.normal(0, 5, n_samples)\n",
    "    df['current'] += np.random.normal(0, 0.5, n_samples)\n",
    "    df['noise_level'] += np.random.normal(0, 2, n_samples)\n",
    "    df['oil_quality'] += np.random.normal(0, 5, n_samples)\n",
    "    df['wear_rate'] += np.random.normal(0, 0.002, n_samples)\n",
    "    \n",
    "    # Clip values to ensure physical realism\n",
    "    df['temperature'] = df['temperature'].clip(lower=0)\n",
    "    df['vibration'] = df['vibration'].clip(lower=0)\n",
    "    df['pressure'] = df['pressure'].clip(lower=0)\n",
    "    df['runtime_hours'] = df['runtime_hours'].clip(lower=0)\n",
    "    df['current'] = df['current'].clip(lower=0)\n",
    "    df['noise_level'] = df['noise_level'].clip(lower=0)\n",
    "    df['oil_quality'] = df['oil_quality'].clip(lower=0, upper=100)\n",
    "    df['wear_rate'] = df['wear_rate'].clip(lower=0)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Dataset saved to {save_path}\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Failure rate: {df['failure'].mean():.2%}\")\n",
    "    print(\"\\nDataset columns:\", list(df.columns))\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = generate_enhanced_dataset()\n",
    "    print(\"\\nFirst few rows of the dataset:\")\n",
    "    print(dataset.head())\n",
    "    print(\"\\nBasic statistics:\")\n",
    "    print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "159d6eb3-4524-4483-81fe-a82261b69170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n",
      "\n",
      "XGBoost Model Evaluation:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.53      0.64       892\n",
      "           1       0.71      0.91      0.79      1108\n",
      "\n",
      "    accuracy                           0.74      2000\n",
      "   macro avg       0.76      0.72      0.72      2000\n",
      "weighted avg       0.76      0.74      0.73      2000\n",
      "\n",
      "ROC-AUC Score: 0.7567\n",
      "Confusion matrix saved as 'confusion_matrix.png'\n",
      "Feature importance plot saved as 'feature_importance.png'\n",
      "\n",
      "Prediction for new equipment data:\n",
      "Failure Probability: 4.19%\n",
      "Recommendation: Equipment appears stable.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_and_preprocess_data(file_path=\"enhanced_predictive_maintenance_dataset.csv\"):\n",
    "    \"\"\"\n",
    "    Load and preprocess the enhanced dataset for XGBoost.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the dataset CSV\n",
    "    \n",
    "    Returns:\n",
    "    Processed data, scaler, and feature names\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found. Please generate the dataset using generate_enhanced_dataset.py.\")\n",
    "        return None, None, None, None, None, None\n",
    "    \n",
    "    # Select features and target\n",
    "    features = ['temperature', 'vibration', 'pressure', 'runtime_hours', \n",
    "                'current', 'noise_level', 'oil_quality', 'wear_rate']\n",
    "    X = df[features]\n",
    "    y = df['failure']\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, features\n",
    "\n",
    "def train_xgboost_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train an XGBoost classifier with optimized parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train: Training features\n",
    "    y_train: Training labels\n",
    "    \n",
    "    Returns:\n",
    "    Trained XGBoost model\n",
    "    \"\"\"\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the XGBoost model with comprehensive metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained XGBoost model\n",
    "    X_test: Test features\n",
    "    y_test: Test labels\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(\"\\nXGBoost Model Evaluation:\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    print(\"Confusion matrix saved as 'confusion_matrix.png'\")\n",
    "\n",
    "def plot_feature_importance(model, features):\n",
    "    \"\"\"\n",
    "    Plot feature importance for the XGBoost model.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained XGBoost model\n",
    "    features: List of feature names\n",
    "    \"\"\"\n",
    "    importance = model.feature_importances_\n",
    "    feature_importance = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
    "    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "    plt.title('XGBoost Feature Importance')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png')\n",
    "    plt.close()\n",
    "    print(\"Feature importance plot saved as 'feature_importance.png'\")\n",
    "\n",
    "def predict_new_data(model, scaler, new_data, features):\n",
    "    \"\"\"\n",
    "    Predict failure probability for new sensor data.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained XGBoost model\n",
    "    scaler: Fitted scaler\n",
    "    new_data: New sensor data as a list or array\n",
    "    features: List of feature names\n",
    "    \"\"\"\n",
    "    new_data_df = pd.DataFrame([new_data], columns=features)\n",
    "    new_data_scaled = scaler.transform(new_data_df)\n",
    "    failure_prob = model.predict_proba(new_data_scaled)[0][1]\n",
    "    \n",
    "    print(f\"\\nPrediction for new equipment data:\")\n",
    "    print(f\"Failure Probability: {failure_prob:.2%}\")\n",
    "    if failure_prob > 0.7:\n",
    "        print(\"Recommendation: Schedule maintenance immediately.\")\n",
    "    elif failure_prob > 0.3:\n",
    "        print(\"Recommendation: Monitor closely and consider maintenance soon.\")\n",
    "    else:\n",
    "        print(\"Recommendation: Equipment appears stable.\")\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    X_train, X_test, y_train, y_test, scaler, features = load_and_preprocess_data()\n",
    "    if X_train is None:\n",
    "        return\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training XGBoost model...\")\n",
    "    xgb_model = train_xgboost_model(X_train, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    evaluate_model(xgb_model, X_test, y_test)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plot_feature_importance(xgb_model, features)\n",
    "    \n",
    "    # Example prediction for new sensor data\n",
    "    # [temp, vib, press, runtime, curr, noise, oil, wear]\n",
    "    new_data = [78, 0.65, 115, 1300, 17, 70, 65, 0.03]\n",
    "    predict_new_data(xgb_model, scaler, new_data, features)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9766b6e-c7fb-4107-8ce7-0fd895500f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n",
      "\n",
      "XGBoost Model Evaluation:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.53      0.64       892\n",
      "           1       0.71      0.91      0.79      1108\n",
      "\n",
      "    accuracy                           0.74      2000\n",
      "   macro avg       0.76      0.72      0.72      2000\n",
      "weighted avg       0.76      0.74      0.73      2000\n",
      "\n",
      "ROC-AUC Score: 0.7567\n",
      "\n",
      "Model saved to xgboost_model.pkl\n",
      "Scaler saved to scaler.pkl\n",
      "\n",
      "Prediction for new equipment data using loaded model:\n",
      "Failure Probability: 4.19%\n",
      "Recommendation: Equipment appears stable.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_and_preprocess_data(file_path=\"enhanced_predictive_maintenance_dataset.csv\"):\n",
    "    \"\"\"\n",
    "    Load and preprocess the enhanced dataset for XGBoost.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the dataset CSV\n",
    "    \n",
    "    Returns:\n",
    "    Processed data, scaler, and feature names\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found. Please generate the dataset using generate_enhanced_dataset.py.\")\n",
    "        return None, None, None, None, None, None\n",
    "    \n",
    "    # Select features and target\n",
    "    features = ['temperature', 'vibration', 'pressure', 'runtime_hours', \n",
    "                'current', 'noise_level', 'oil_quality', 'wear_rate']\n",
    "    X = df[features]\n",
    "    y = df['failure']\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, features\n",
    "\n",
    "def train_xgboost_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train an XGBoost classifier with optimized parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train: Training features\n",
    "    y_train: Training labels\n",
    "    \n",
    "    Returns:\n",
    "    Trained XGBoost model\n",
    "    \"\"\"\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the XGBoost model with comprehensive metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained XGBoost model\n",
    "    X_test: Test features\n",
    "    y_test: Test labels\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(\"\\nXGBoost Model Evaluation:\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "def save_model_and_scaler(model, scaler, model_path=\"xgboost_model.pkl\", scaler_path=\"scaler.pkl\"):\n",
    "    \"\"\"\n",
    "    Save the trained model and scaler as pickle files.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained XGBoost model\n",
    "    scaler: Fitted scaler\n",
    "    model_path (str): Path to save the model pickle file\n",
    "    scaler_path (str): Path to save the scaler pickle file\n",
    "    \"\"\"\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f\"\\nModel saved to {model_path}\")\n",
    "    print(f\"Scaler saved to {scaler_path}\")\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    X_train, X_test, y_train, y_test, scaler, features = load_and_preprocess_data()\n",
    "    if X_train is None:\n",
    "        return\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training XGBoost model...\")\n",
    "    xgb_model = train_xgboost_model(X_train, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    evaluate_model(xgb_model, X_test, y_test)\n",
    "    \n",
    "    # Save model and scaler to pickle files\n",
    "    save_model_and_scaler(xgb_model, scaler)\n",
    "    \n",
    "    # Demonstrate loading and predicting with saved model\n",
    "    with open(\"xgboost_model.pkl\", 'rb') as f:\n",
    "        loaded_model = pickle.load(f)\n",
    "    with open(\"scaler.pkl\", 'rb') as f:\n",
    "        loaded_scaler = pickle.load(f)\n",
    "    \n",
    "    # Example prediction with loaded model\n",
    "    new_data = [78, 0.65, 115, 1300, 17, 70, 65, 0.03]  # [temp, vib, press, runtime, curr, noise, oil, wear]\n",
    "    new_data_df = pd.DataFrame([new_data], columns=features)\n",
    "    new_data_scaled = loaded_scaler.transform(new_data_df)\n",
    "    failure_prob = loaded_model.predict_proba(new_data_scaled)[0][1]\n",
    "    \n",
    "    print(f\"\\nPrediction for new equipment data using loaded model:\")\n",
    "    print(f\"Failure Probability: {failure_prob:.2%}\")\n",
    "    if failure_prob > 0.7:\n",
    "        print(\"Recommendation: Schedule maintenance immediately.\")\n",
    "    elif failure_prob > 0.3:\n",
    "        print(\"Recommendation: Monitor closely and consider maintenance soon.\")\n",
    "    else:\n",
    "        print(\"Recommendation: Equipment appears stable.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce3038b-2d7f-478d-a1d3-fd3f0a381a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
